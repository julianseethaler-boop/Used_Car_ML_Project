{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":272138484,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lineare Regression & Regularisierung\n\nIn diesem Notebook baue ich ein erstes **lineares Modell** zur Vorhersage des Verkaufspreises von Gebrauchtwagen.  \nZiel ist es, eine **interpretierbare Baseline** zu erstellen und zu untersuchen, wie gut sich der Preis mit einer (regularisierten) linearen Regression erklären lässt.\n\nIm Gegensatz zu komplexeren Modellen (z. B. Gradient Boosting) liegt der Fokus hier weniger auf maximaler Performance, sondern auf dem Gewinnen von Einblicken in die Preisbildung der Gebrauchtwagen.","metadata":{}},{"cell_type":"markdown","source":"## Gliederung\n\nIn diesem Notebook gehe ich wie folgt vor:\n\n### 1. Daten laden & Überblick\n- Importieren der benötigten Bibliotheken\n- Laden des vorbereiteten Datensatzes\n- Kurzer Überblick über den Datensatz\n\n### 2. Baseline-Modell (OLS)\n- Definition einer Funktion für lineare Regression\n- Auswahl eines Feature-Sets durch iteratives Entfernen\n\n### 3. Erstellung neuer Features\n- RTO-Features\n- Car Rating als numerisches Feature\n- Interaktionsfeatures\n\n### 4. Binning kategorialer Features\n- Motivation und Einordnung des Binning-Ansatzes\n- Definition einer Funktion zur Bestimmung geeigneter Bins\n- Anwendung des Binning auf ausgewählte Features\n\n### 5. Logarithmierung des Targets\n- Motivation für die Log-Transformation\n- Definition einer linearen Regression mit logarithmiertem Target\n- Iteratives Hinzufügen von Features\n\n### 6. Ridge-Regression\n- Implementierung einer Ridge-Regression\n- Hyperparameter-Tuning\n- Feature-Selection unter Regularisierung\n\n### 7. Übersicht & Interpretation\n- Vergleich der Modellperformance\n- Interpretation kategorialer Koeffizienten\n- Interpretation numerischer Koeffizienten\n\n### 8. Fazit\n- Zusammenfassung der Modellschritte\n- Einordnung der Ergebnisse\n- Ableitung einer linearen Baseline für weiterführende Modelle\n\nAm Ende dieses Notebooks steht eine gut dokumentierte, interpretierbare lineare Baseline, \ndie als Referenz für komplexere Modelle in den folgenden Schritten dient.\n","metadata":{}},{"cell_type":"markdown","source":"## Daten Laden und Überblick\n#### Importieren der benötigten Bibliotheken","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n\n# %config InlineBackend.figure_format = 'retina'\n# plt.style.use('seaborn-whitegrid')\n# plt.rc('figure', autolayout = True, dpi = 110)\n# plt.rc('axes', labelweight = 'bold', titlesize = 14, titleweight = 'bold')\n# plt.rc('font', size = 11)\n\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.compose import ColumnTransformer, make_column_selector as Selector\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, Lasso\nfrom prettytable import PrettyTable\nfrom sklearn.pipeline import Pipeline\nimport time\nimport random\n\nrandom.seed(42)\nnp.random.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:19:25.571954Z","iopub.execute_input":"2025-12-10T10:19:25.572238Z","iopub.status.idle":"2025-12-10T10:19:30.839886Z","shell.execute_reply.started":"2025-12-10T10:19:25.572199Z","shell.execute_reply":"2025-12-10T10:19:30.838442Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/used-car-prediction-notebook-02/cleaned_data_02.csv\n/kaggle/input/used-car-prediction-notebook-02/__results__.html\n/kaggle/input/used-car-prediction-notebook-02/__notebook__.ipynb\n/kaggle/input/used-car-prediction-notebook-02/__output__.json\n/kaggle/input/used-car-prediction-notebook-02/custom.css\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___11_0.png\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___24_0.png\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___19_1.png\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___10_1.png\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___7_0.png\n/kaggle/input/used-car-prediction-notebook-02/__results___files/__results___13_0.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### Laden des vorbereiteten Datensatzes + Kurzer Blick","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/used-car-prediction-notebook-02/cleaned_data_02.csv')\nprint(df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:19:37.399924Z","iopub.execute_input":"2025-12-10T10:19:37.400415Z","iopub.status.idle":"2025-12-10T10:19:37.481800Z","shell.execute_reply.started":"2025-12-10T10:19:37.400378Z","shell.execute_reply":"2025-12-10T10:19:37.480913Z"}},"outputs":[{"name":"stdout","text":"(7391, 17)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  fuel_type  kms_run  sale_price   city  body_type transmission  \\\n0    petrol     8063      386399  noida  hatchback       manual   \n1    petrol    23104      265499  noida  hatchback       manual   \n2    petrol    23402      477699  noida  hatchback       manual   \n3    diesel    39124      307999  noida  hatchback       manual   \n4    petrol    22116      361499  noida  hatchback       manual   \n\n           variant registered_city registered_state   rto     make      model  \\\n0          lxi opt           delhi            delhi  dl6c   maruti      swift   \n1              lxi           noida    uttar pradesh  up16   maruti   alto 800   \n2  sports 1.2 vtvt            agra    uttar pradesh  up80  hyundai  grand i10   \n3              vdi           delhi            delhi  dl1c   maruti      swift   \n4   magna 1.2 vtvt       new delhi            delhi  dl12  hyundai  grand i10   \n\n   total_owners car_rating  fitness_certificate  warranty_avail  age  \n0             2      great                 True           False    6  \n1             1      great                 True           False    5  \n2             1      great                 True           False    4  \n3             1      great                 True           False    8  \n4             1      great                 True           False    6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fuel_type</th>\n      <th>kms_run</th>\n      <th>sale_price</th>\n      <th>city</th>\n      <th>body_type</th>\n      <th>transmission</th>\n      <th>variant</th>\n      <th>registered_city</th>\n      <th>registered_state</th>\n      <th>rto</th>\n      <th>make</th>\n      <th>model</th>\n      <th>total_owners</th>\n      <th>car_rating</th>\n      <th>fitness_certificate</th>\n      <th>warranty_avail</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>petrol</td>\n      <td>8063</td>\n      <td>386399</td>\n      <td>noida</td>\n      <td>hatchback</td>\n      <td>manual</td>\n      <td>lxi opt</td>\n      <td>delhi</td>\n      <td>delhi</td>\n      <td>dl6c</td>\n      <td>maruti</td>\n      <td>swift</td>\n      <td>2</td>\n      <td>great</td>\n      <td>True</td>\n      <td>False</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>petrol</td>\n      <td>23104</td>\n      <td>265499</td>\n      <td>noida</td>\n      <td>hatchback</td>\n      <td>manual</td>\n      <td>lxi</td>\n      <td>noida</td>\n      <td>uttar pradesh</td>\n      <td>up16</td>\n      <td>maruti</td>\n      <td>alto 800</td>\n      <td>1</td>\n      <td>great</td>\n      <td>True</td>\n      <td>False</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>petrol</td>\n      <td>23402</td>\n      <td>477699</td>\n      <td>noida</td>\n      <td>hatchback</td>\n      <td>manual</td>\n      <td>sports 1.2 vtvt</td>\n      <td>agra</td>\n      <td>uttar pradesh</td>\n      <td>up80</td>\n      <td>hyundai</td>\n      <td>grand i10</td>\n      <td>1</td>\n      <td>great</td>\n      <td>True</td>\n      <td>False</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>diesel</td>\n      <td>39124</td>\n      <td>307999</td>\n      <td>noida</td>\n      <td>hatchback</td>\n      <td>manual</td>\n      <td>vdi</td>\n      <td>delhi</td>\n      <td>delhi</td>\n      <td>dl1c</td>\n      <td>maruti</td>\n      <td>swift</td>\n      <td>1</td>\n      <td>great</td>\n      <td>True</td>\n      <td>False</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petrol</td>\n      <td>22116</td>\n      <td>361499</td>\n      <td>noida</td>\n      <td>hatchback</td>\n      <td>manual</td>\n      <td>magna 1.2 vtvt</td>\n      <td>new delhi</td>\n      <td>delhi</td>\n      <td>dl12</td>\n      <td>hyundai</td>\n      <td>grand i10</td>\n      <td>1</td>\n      <td>great</td>\n      <td>True</td>\n      <td>False</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Baseline-Modell (OLS)\n\n#### Definition einer Funktion für lineare Regression\n\nUm neue Features effizient bewerten zu können, definiere ich zunächst eine kompakte\nBaseline-Regression, die automatisch One-Hot-Encoding und Skalierung übernimmt.\nDie Funktion liefert einen schnellen Cross-Validation-Score, mit dem ich\nverschiedene Feature-Sets vergleichen kann.\n","metadata":{}},{"cell_type":"code","source":"def get_regression(data, return_mae = False):\n    X = data.copy()\n    y = X.pop('sale_price')\n    preprocessor = ColumnTransformer([\n        ('num', StandardScaler(), Selector(dtype_include = 'number')),\n        ('cat', OneHotEncoder(drop = 'first', handle_unknown = 'ignore'), Selector(dtype_include = 'object'))\n    ])\n    \n    pipe = make_pipeline(preprocessor, LinearRegression())\n\n    score_r2 = cross_val_score(pipe, X, y, cv = 5)\n    \n    if return_mae:\n        score_mae = cross_val_score(pipe, X, y, cv = 5, scoring = 'neg_mean_absolute_error')\n        score_mae = - score_mae\n        return score_r2.mean(), score_mae.mean()\n    else:\n        return score_r2.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:19:41.298004Z","iopub.execute_input":"2025-12-10T10:19:41.298765Z","iopub.status.idle":"2025-12-10T10:19:41.305600Z","shell.execute_reply.started":"2025-12-10T10:19:41.298736Z","shell.execute_reply":"2025-12-10T10:19:41.304563Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"get_regression(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:19:43.872823Z","iopub.execute_input":"2025-12-10T10:19:43.873160Z","iopub.status.idle":"2025-12-10T10:19:46.519124Z","shell.execute_reply.started":"2025-12-10T10:19:43.873133Z","shell.execute_reply":"2025-12-10T10:19:46.517579Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0.8714224843407303"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"#### Auswahl eines Featureset durch iteratives Entfernen\nEine simple Methode für die Auswahl eines kompakten Featuresets, ist es, je ein Feature zu entfernen und darauf die Regression zu trainieren. Dasjenige Feature, dessen Elimination zum höchsten Performancezuwachs führt, wird entfernt. Dies wird wiederholt bis keine Zuwächse mehr erreicht werden können. Dies führt zu einem lokalen Optimun, was hier genügen soll.","metadata":{}},{"cell_type":"code","source":"for feature in df.drop(columns = 'sale_price').columns:\n    print('Ohne', feature, ':', get_regression(df.drop(columns = feature)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:19:55.881040Z","iopub.execute_input":"2025-12-10T10:19:55.881437Z","iopub.status.idle":"2025-12-10T10:20:29.980394Z","shell.execute_reply.started":"2025-12-10T10:19:55.881355Z","shell.execute_reply":"2025-12-10T10:20:29.979387Z"}},"outputs":[{"name":"stdout","text":"Ohne fuel_type : 0.8714508776709147\nOhne kms_run : 0.8667700815301058\nOhne city : 0.8719879582126291\nOhne body_type : 0.8723362003473909\nOhne transmission : 0.8693197700678947\nOhne variant : 0.823073951316189\nOhne registered_city : 0.873842833884044\nOhne registered_state : 0.8702746872931117\nOhne rto : 0.8741452198919321\nOhne make : 0.8544426878421213\nOhne model : 0.8366727529876815\nOhne total_owners : 0.8699198283943457\nOhne car_rating : 0.8696167029456469\nOhne fitness_certificate : 0.8714224843407303\nOhne warranty_avail : 0.8714224843407303\nOhne age : 0.8226366935078024\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"base = ['kms_run', 'sale_price', 'transmission', 'variant', 'registered_state', 'make', 'model', 'total_owners', 'car_rating', 'age']\nresult_1 = get_regression(df[base], return_mae = True)\nresult_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:20:35.081429Z","iopub.execute_input":"2025-12-10T10:20:35.081835Z","iopub.status.idle":"2025-12-10T10:20:36.802637Z","shell.execute_reply.started":"2025-12-10T10:20:35.081809Z","shell.execute_reply":"2025-12-10T10:20:36.801641Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(0.8775427832790594, 49339.62341243577)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Weiter Iterationen ergaben ein lokales Optimum der Regression mit den Features: **'kms_run', 'sale_price', 'transmission', 'variant', 'registered_state', 'make', 'model', 'total_owners', 'car_rating', 'age'**.","metadata":{}},{"cell_type":"markdown","source":"## Erstellung neuer Features\n","metadata":{}},{"cell_type":"code","source":"#Kopie des Datensatzes. Änderungen werden auf der Kopie durchgeführt.\ndf_fe = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:21:29.806256Z","iopub.execute_input":"2025-12-10T10:21:29.806700Z","iopub.status.idle":"2025-12-10T10:21:29.814841Z","shell.execute_reply.started":"2025-12-10T10:21:29.806664Z","shell.execute_reply":"2025-12-10T10:21:29.813348Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#### RTO-Features\nDie RTO-Kennung besteht aus zwei Buchstaben (Bundesstaat) und einer Zahlenfolge. Um die hohe Kardinalität zu reduzieren, trenne ich die Variable in zwei Teile:  \n\n- **rto_state**: zweibuchstabiges Zeichen für Bundesstaat ähnlich zu registered_state\n- **rto_number**: numerische weitere Unterteilung der Bundestaaten, vermutlich weniger relevant","metadata":{}},{"cell_type":"code","source":"df_fe['rto_state'] = df_fe['rto'].astype(str).str[:2]\ndf_fe['rto_number'] = df_fe['rto'].astype(str).str[2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:47:38.245990Z","iopub.execute_input":"2025-12-10T10:47:38.246354Z","iopub.status.idle":"2025-12-10T10:47:38.259004Z","shell.execute_reply.started":"2025-12-10T10:47:38.246329Z","shell.execute_reply":"2025-12-10T10:47:38.257058Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"features = base + ['rto_state', 'rto_number']\nprint(get_regression(df[base]))\nprint(get_regression(df_fe[features]))\nprint(get_regression(df_fe[features].drop(columns = 'rto_number')))\nprint(get_regression(df_fe[features].drop(columns = 'rto_state')))\nprint(get_regression(df_fe[features].drop(columns = 'registered_state')))\nprint(get_regression(df_fe[features].drop(columns = ['rto_number', 'registered_state'])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:21:45.150150Z","iopub.execute_input":"2025-12-10T10:21:45.151014Z","iopub.status.idle":"2025-12-10T10:21:51.240584Z","shell.execute_reply.started":"2025-12-10T10:21:45.150980Z","shell.execute_reply":"2025-12-10T10:21:51.239383Z"}},"outputs":[{"name":"stdout","text":"0.8775427832790594\n0.876182946825538\n0.8776949696628454\n0.8759711814788961\n0.8759560465734328\n0.8776711423033969\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"rto_number scheint überflüssig zu sein. Mit registered_state und rto_state performt das Modell minimal besser.","metadata":{}},{"cell_type":"markdown","source":"#### Car_rating als numerisches Feature\nDa car_rating eine logische Reihenfolge besitzt, stelle ich car_rating als Integer da.","metadata":{}},{"cell_type":"code","source":"df_fe['car_rating_number'] = df_fe.car_rating.map({'overpriced' : 1, 'fair' : 2, 'good' : 3, 'great' : 4})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:21:58.049986Z","iopub.execute_input":"2025-12-10T10:21:58.050298Z","iopub.status.idle":"2025-12-10T10:21:58.057765Z","shell.execute_reply.started":"2025-12-10T10:21:58.050275Z","shell.execute_reply":"2025-12-10T10:21:58.056509Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"features = base + ['car_rating_number']\nprint(get_regression(df[base]))\nprint(get_regression(df_fe[features]))\nprint(get_regression(df_fe[features].drop(columns = 'car_rating')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:22:00.470145Z","iopub.execute_input":"2025-12-10T10:22:00.470908Z","iopub.status.idle":"2025-12-10T10:22:03.422950Z","shell.execute_reply.started":"2025-12-10T10:22:00.470856Z","shell.execute_reply":"2025-12-10T10:22:03.421543Z"}},"outputs":[{"name":"stdout","text":"0.8775427832790594\n0.8775428870095752\n0.877358692018241\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Auch Car_rating_number verbessert die lineare Regression noch nicht nennenswert.","metadata":{}},{"cell_type":"markdown","source":"#### Interaktionsfeatures\nInteraktionsfeatures ermöglichen es linearen Modellen komplexe Zusammenhänge besser zu erfassen. Erstellen wir einge Verhältnisse, wobei eine Vielzahl an anderen Interaktionen denkbar sind.","metadata":{}},{"cell_type":"code","source":"#KM pro Besitzer\ndf_fe['kms_per_owner'] = df_fe['kms_run'] / df_fe['total_owners']\n\n#KM pro Jahr/ Wie viel wird Auto gefahren?\ndf_fe['kms_per_age'] = np.where(df_fe['age'] == 0, df_fe['kms_run'], df_fe['kms_run'] / df_fe['age'])\n\n#Durchschnittliche KM pro Modell\ndf_fe['avg_kms_by_model'] = df_fe.groupby('model')['kms_run'].transform('mean')\n\n#KM im Vergleich zu anderen Autos des gleichen Modells\ndf_fe['kms_vs_model_avg'] = df_fe['kms_run'] / df_fe['avg_kms_by_model']\n\n# Durchschnittliches Alter pro Modell\ndf_fe['avg_age_by_model'] = df_fe.groupby('model')['age'].transform('mean')\n\n#Alter im Vergleich zu anderen Autos des gleichen Modells\ndf_fe['age_vs_model_avg'] = df_fe['age'] / df_fe['avg_age_by_model']\n\n#Druchschnittliches Rating pro Modell\ndf_fe['avg_rating_by_model'] = df_fe.groupby('model')['car_rating_number'].transform('mean')\n\n#Rating im Vergleich zu anderen Autos des gleichen Modells\ndf_fe['rating_vs_model_avg'] = df_fe['car_rating_number'] / df_fe['avg_rating_by_model']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:29:25.894449Z","iopub.execute_input":"2025-12-10T10:29:25.895144Z","iopub.status.idle":"2025-12-10T10:29:25.919663Z","shell.execute_reply.started":"2025-12-10T10:29:25.895105Z","shell.execute_reply":"2025-12-10T10:29:25.918414Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"ia_features =  ['kms_per_owner', 'kms_per_age', 'avg_kms_by_model', 'kms_vs_model_avg',\n       'avg_age_by_model', 'age_vs_model_avg', 'avg_rating_by_model', 'rating_vs_model_avg']\n\nprint(result_1)\nfor i in ia_features:\n    features = base + [i]\n    print(i, ':', get_regression(df_fe[features]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:29:30.485528Z","iopub.execute_input":"2025-12-10T10:29:30.485838Z","iopub.status.idle":"2025-12-10T10:29:37.860123Z","shell.execute_reply.started":"2025-12-10T10:29:30.485816Z","shell.execute_reply":"2025-12-10T10:29:37.859075Z"}},"outputs":[{"name":"stdout","text":"(0.8775427832790594, 49339.62341243577)\nkms_per_owner : 0.8775208672180472\nkms_per_age : 0.8775126036885006\navg_kms_by_model : 0.8771330582039226\nkms_vs_model_avg : 0.8780804361897315\navg_age_by_model : 0.878237259684718\nage_vs_model_avg : 0.8770884446639062\navg_rating_by_model : 0.8778304927555404\nrating_vs_model_avg : 0.8776138925935253\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Iteratives Hinzufügen ergabe ein lokales Optimum mit folgenden zusätzlichen Features:  \n\n**'kms_vs_model_avg'**, **'avg_age_by_model'**, **'age_vs_model_avg'**  ","metadata":{}},{"cell_type":"code","source":"features = base + ['kms_vs_model_avg', 'avg_age_by_model','age_vs_model_avg']\nresult_2 = get_regression(df_fe[features], return_mae = True)\nresult_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:29:48.535180Z","iopub.execute_input":"2025-12-10T10:29:48.535528Z","iopub.status.idle":"2025-12-10T10:29:50.600648Z","shell.execute_reply.started":"2025-12-10T10:29:48.535506Z","shell.execute_reply":"2025-12-10T10:29:50.599822Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(0.8787767844400207, 49051.63764036611)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Der Nutzen unserer Zusätzlichen Features bleibt insgesamt relativ gering für unsere einfache Lineare Regression. Dies kann sich jedoch bei den folgenden modifizierten Regressionen ändern.","metadata":{}},{"cell_type":"markdown","source":"## Binning kategorischer Features\n\n#### Defnintion einer Funktion für optimale Binsize pro Feature\n\nHier versuche ich die Modelperformance zu verbessern, indem ich systemsatisch optimale Grenzen finde, bei denen ich die Kategorien zusammenfasse zu einer neuen Kategorie \"OTHER\". Dafür wird ein Feature ausgewählt und für unterschiedliche Tresholds je eine Regression trainiert, wobei aus dem höchsten Score die optimale Grenze geschlossen wird.","metadata":{}},{"cell_type":"code","source":"def optimal_bin(df, feature, max_thr = 30):\n    table = PrettyTable()\n    table.field_names = ['min_bin_size', 'r2']\n    \n    vc = df[feature].value_counts()\n    thresholds = sorted(vc[vc <= max_thr].unique())\n    \n    for t in thresholds:\n        df_base = df.copy()\n        rare = vc[vc < t].index\n        df_base[feature] = np.where(df_base[feature].isin(rare), 'OTHER', df_base[feature])\n        r2 = get_regression(df_base[features])\n        table.add_row([t, r2])\n        \n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:35:43.904466Z","iopub.execute_input":"2025-12-10T10:35:43.905615Z","iopub.status.idle":"2025-12-10T10:35:43.913859Z","shell.execute_reply.started":"2025-12-10T10:35:43.905565Z","shell.execute_reply":"2025-12-10T10:35:43.911689Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"optimal_bin(df_fe, 'make')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:35:50.611852Z","iopub.execute_input":"2025-12-10T10:35:50.612289Z","iopub.status.idle":"2025-12-10T10:35:57.749883Z","shell.execute_reply.started":"2025-12-10T10:35:50.612261Z","shell.execute_reply":"2025-12-10T10:35:57.748686Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"+--------------+--------------------+\n| min_bin_size |         r2         |\n+--------------+--------------------+\n|      1       | 0.8787767844400207 |\n|      2       | 0.8813816998952759 |\n|      3       | 0.8813620488422712 |\n|      9       | 0.8813420382673629 |\n|      13      | 0.8809755416282865 |\n|      15      | 0.8810593334015373 |\n|      27      | 0.8796975264211998 |\n+--------------+--------------------+","text/html":"<table>\n    <thead>\n        <tr>\n            <th>min_bin_size</th>\n            <th>r2</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>1</td>\n            <td>0.8787767844400207</td>\n        </tr>\n        <tr>\n            <td>2</td>\n            <td>0.8813816998952759</td>\n        </tr>\n        <tr>\n            <td>3</td>\n            <td>0.8813620488422712</td>\n        </tr>\n        <tr>\n            <td>9</td>\n            <td>0.8813420382673629</td>\n        </tr>\n        <tr>\n            <td>13</td>\n            <td>0.8809755416282865</td>\n        </tr>\n        <tr>\n            <td>15</td>\n            <td>0.8810593334015373</td>\n        </tr>\n        <tr>\n            <td>27</td>\n            <td>0.8796975264211998</td>\n        </tr>\n    </tbody>\n</table>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Für das Feature 'make' erreichen wir den besten R2-Wert, wenn wir lediglich Kategorien mit je einer Ausprägung zusammenfassen.  \nAnwendung der Funktion auf kategorische Features ergab folgende optimale Tresholds:\n- model : 2\n- registered_state : 22\n- make : 2\n- variant: 1\n\nBei variant ist also die minimale Anzahl die eine Kategorie haben muss um nicht zu OTHER zusammengefasst zu werden 1. Dies ist überraschend, da das Model bei einer Kategorie, die im Testsatz einmal vorkommt, keine Informationen aus dem Trainingssatz ziehen kann. Der Performanceverlust scheint also durch Leakage zu entstehen. Mögliche Lösungen wäre es Gruppen nach dem Split zusammenzufassen.","metadata":{}},{"cell_type":"code","source":"# model binnen\ncounts = df_fe.model.value_counts()\nfreq = counts[counts >= 2]\ndf_fe['model'] = df_fe['model'].apply(lambda x: x if x in freq else 'Other')\n\n# registered_state binnen\ncounts = df_fe.registered_state.value_counts()\nfreq = counts[counts >= 22]\ndf_fe['registered_state'] = df_fe.registered_state.apply(lambda x: x if x in freq else 'Other')\n\n# make binnen\ncounts = df_fe.make.value_counts()\nfreq = counts[counts >= 2]\ndf_fe['make'] = df_fe['make'].apply(lambda x: x if x in freq else 'Other')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:36:07.150415Z","iopub.execute_input":"2025-12-10T10:36:07.150723Z","iopub.status.idle":"2025-12-10T10:36:07.187845Z","shell.execute_reply.started":"2025-12-10T10:36:07.150703Z","shell.execute_reply":"2025-12-10T10:36:07.186600Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"result_3 = get_regression(df_fe[features], return_mae = True)\nresult_3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:36:09.367269Z","iopub.execute_input":"2025-12-10T10:36:09.367614Z","iopub.status.idle":"2025-12-10T10:36:11.526715Z","shell.execute_reply.started":"2025-12-10T10:36:09.367592Z","shell.execute_reply":"2025-12-10T10:36:11.525628Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(0.8817931151475952, 48853.21823860336)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Logarithmieren des Targets\nDa Das Target rechtsschief ist, ist zu erwarten, dass logarithmieren eine deutliche Verbesserung bringt.","metadata":{}},{"cell_type":"code","source":"def log_reg(data, cv = 5, return_mae = False):\n    X = data.drop(columns = 'sale_price')\n    y = data.sale_price\n    \n    preprocessor = ColumnTransformer([\n        ('num', StandardScaler(), Selector(dtype_include = 'number')),\n        ('kat', OneHotEncoder(drop = 'first', handle_unknown = 'ignore'), Selector(dtype_include ='object'))\n    ])\n    model = LinearRegression()\n    reg = TransformedTargetRegressor(\n        regressor = model,\n        func = np.log1p,\n        inverse_func = np.expm1\n    )\n    pipe = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', reg)\n    ])\n    score_r2 = cross_val_score(pipe, X, y, cv = cv)\n    if return_mae:\n        score_mae = cross_val_score(pipe, X, y, cv = cv, scoring = 'neg_mean_absolute_error')\n        score_mae = -score_mae\n        return score_r2.mean(), score_mae.mean()\n    return score_r2.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:48:18.029398Z","iopub.execute_input":"2025-12-10T10:48:18.030565Z","iopub.status.idle":"2025-12-10T10:48:18.044160Z","shell.execute_reply.started":"2025-12-10T10:48:18.030525Z","shell.execute_reply":"2025-12-10T10:48:18.041773Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"log_reg(df_fe[base])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:48:22.789552Z","iopub.execute_input":"2025-12-10T10:48:22.790787Z","iopub.status.idle":"2025-12-10T10:48:23.956779Z","shell.execute_reply.started":"2025-12-10T10:48:22.790753Z","shell.execute_reply":"2025-12-10T10:48:23.954720Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.890966247585198"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"#### Iteratives Hinzufügen von Features\nDer Score ist bereits besser, doch die Logarithmierung des Targets kann es ermöglichen noch mehr Features gewinnbringend einzusetzen. Ich füge iterativ Features zu unserem bisherigen Set hinzu, um ein gutes Featureset zu finden.","metadata":{"execution":{"iopub.status.busy":"2025-11-19T00:50:24.884586Z","iopub.execute_input":"2025-11-19T00:50:24.885263Z","iopub.status.idle":"2025-11-19T00:50:24.888615Z","shell.execute_reply.started":"2025-11-19T00:50:24.885240Z","shell.execute_reply":"2025-11-19T00:50:24.887752Z"}}},{"cell_type":"code","source":"out = [c for c in df_fe.columns if c not in base]\nfor o in out:\n    features = base + [o]\n    print(o, ':', log_reg(df_fe[features]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:48:35.660159Z","iopub.execute_input":"2025-12-10T10:48:35.660534Z","iopub.status.idle":"2025-12-10T10:48:55.696881Z","shell.execute_reply.started":"2025-12-10T10:48:35.660505Z","shell.execute_reply":"2025-12-10T10:48:55.695937Z"}},"outputs":[{"name":"stdout","text":"fuel_type : 0.8920300669181902\ncity : 0.8902650210300116\nbody_type : 0.8884987124707564\nregistered_city : 0.8879339404527767\nrto : 0.881159226413718\nfitness_certificate : 0.890966247585198\nwarranty_avail : 0.890966247585198\nrto_state : 0.8919550693839244\nrto_number : 0.8886915828208162\ncar_rating_number : 0.8909681720940889\nkms_per_owner : 0.8909392676722833\nkms_per_age : 0.8907553827559903\navg_kms_by_model : 0.8905757126611968\nkms_vs_model_avg : 0.8904756866158516\navg_age_by_model : 0.8882511954946258\nage_vs_model_avg : 0.8926456798775693\navg_rating_by_model : 0.8906551747391225\nrating_vs_model_avg : 0.8891910048598766\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"Ein Lokoales Optimum ergibt sich somit mit den zusätzlichen Features: \n**'age_vs_model_avg', 'body_type', 'avg_kms_by_model', 'kms_vs_model_avg', 'fuel_type', 'avg_age_by_model'**.","metadata":{}},{"cell_type":"code","source":"opt_features = base + ['fuel_type', 'age_vs_model_avg', 'rto_state', 'kms_per_age']\nresult_4 = log_reg(df_fe[opt_features], return_mae = True)\nresult_4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:48:55.698415Z","iopub.execute_input":"2025-12-10T10:48:55.699013Z","iopub.status.idle":"2025-12-10T10:48:57.974418Z","shell.execute_reply.started":"2025-12-10T10:48:55.698977Z","shell.execute_reply":"2025-12-10T10:48:57.973144Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(0.8952131553926558, 47025.02912938857)"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Regularierte Modelle\nDurch die Einführung eines Strafterms verringert Regularisierung Overfitting auf den Trainingsdaten. Durch eine verbesserte Generalisierung kann oft eine bessere Erklärungkraft erzielt werden. Dies gilt insbesondere für Daten mit hochkardinalen Features, für uns also perfekt.","metadata":{}},{"cell_type":"markdown","source":"#### Funktion für Ridge-Regression\nIm nächsten Schritt erweitere ich das lineare Modell um eine L2-Regularisierung (Ridge). Dabei wird ein Strafterm eingeführt, was die Koeffizienten reduziert und zu geringerem Overfitting führt, insbesondere bei stark korrelierten Features.\n\nDie folgende Funktion trainiert das Ridge-Modell und kann auf Wunsch zusätzlich die Referenzkategorien unserer kategorischen Features, sowie die Modellkoeffizienten zurückgeben.","metadata":{}},{"cell_type":"code","source":"def get_ridge(data, alpha = 1, return_reference = False, return_coef = False, return_mae = False):\n    X = data.copy()\n    y = X.pop('sale_price')\n    preprocessor = ColumnTransformer([\n        ('num', StandardScaler(), Selector(dtype_include = 'number')),\n        ('cat', OneHotEncoder(drop = 'first', handle_unknown = 'ignore', sparse_output=False),\n         Selector(dtype_include = 'object'))\n    ])\n    \n    ridge = Ridge(alpha = alpha)\n\n    reg = TransformedTargetRegressor(\n        regressor=ridge,\n        func=np.log1p,\n        inverse_func=np.expm1,\n    )\n\n    pipe = make_pipeline(preprocessor, reg)\n\n    score_r2 = cross_val_score(pipe, X, y, cv=5)\n    \n    if return_reference | return_coef:\n        pipe.fit(X,y)\n        pre = pipe.named_steps['columntransformer']\n            \n    if return_reference:\n        ohe = pre.named_transformers_['cat']\n        cat_cols = X.select_dtypes('object').columns\n\n        print('Referenzkategorien:')\n        reference = []\n        for col, cats in zip(cat_cols, ohe.categories_):\n            ref = cats[0]\n            dummies = list(cats[1:])\n            reference.append((col, ref))\n        reference_df = pd.DataFrame(reference, columns = ['Feature', 'Referenzkategorie'])\n        reference_df.index += 1\n        \n            \n    if return_coef:\n        feature_names = pre.get_feature_names_out()\n    \n        tt = pipe.named_steps['transformedtargetregressor']\n        ridge = tt.regressor_\n        coef = pd.Series(ridge.coef_, index=feature_names)\n    \n        important = coef.reindex(coef.abs().sort_values(ascending=False).index)\n        return important\n        \n    elif return_reference:\n        return reference_df\n        \n    elif return_mae:\n        score_mae = cross_val_score(pipe, X, y, cv = 5, scoring = 'neg_mean_absolute_error')\n        score_mae = -score_mae\n        return score_r2.mean(), score_mae.mean()\n        \n    else:\n        return scores.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:50:53.620916Z","iopub.execute_input":"2025-12-10T10:50:53.621419Z","iopub.status.idle":"2025-12-10T10:50:53.632638Z","shell.execute_reply.started":"2025-12-10T10:50:53.621392Z","shell.execute_reply":"2025-12-10T10:50:53.631513Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"#### Funktion für Hyperparameter\nDie folgende Funktion verwednet GridSearchCV, um den optimalen Wert für den Hyperparameter **alpha** für unsere Regression zu ermitteln, dabei ist es möglich einen oberen und unteren startwert, sowie die Anzahl der Werte, die probiert werden anzugeben.","metadata":{}},{"cell_type":"code","source":"def get_alpha_ridge(data, low = -4, high = 1, n = 50):\n    X = data.copy()\n    y = X.pop('sale_price')\n\n    preprocessor = ColumnTransformer([\n        ('num', StandardScaler(), Selector(dtype_include = 'number')),\n        ('car', OneHotEncoder(drop = 'first', handle_unknown = 'ignore', sparse_output = False),\n         Selector(dtype_include = 'object'))\n    ])\n    \n    regressor = Ridge()\n    reg = TransformedTargetRegressor(\n        regressor = regressor,\n        func = np.log1p,\n        inverse_func = np.expm1\n    )\n\n    pipe = make_pipeline(preprocessor, reg)\n    param_grid = {\n    'transformedtargetregressor__regressor__alpha': np.logspace(low, high, n)\n    }\n    search = GridSearchCV(pipe, param_grid, cv = 5)\n    search.fit(X, y)\n    return search.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:50:57.127731Z","iopub.execute_input":"2025-12-10T10:50:57.128077Z","iopub.status.idle":"2025-12-10T10:50:57.136162Z","shell.execute_reply.started":"2025-12-10T10:50:57.128053Z","shell.execute_reply":"2025-12-10T10:50:57.135071Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"#### Funktion für Feature Selection\nZur Auswahl geeigneter Eingangsdaten verwende ich ein einfaches\nForward-Selection-Verfahren. Ausgehend von einer beliebigen Feature-Basis wird in\njedem Schritt dasjenige Feature ergänzt, das den R²-Wert am stärksten erhöht. Die Schleife endet, sobald kein weiteres\nFeature eine Verbesserung bringt. Die Funktion gibt die finale Feature-Liste\neinschließlich des Targets zurück.","metadata":{}},{"cell_type":"code","source":"def get_features(data, base_features, alpha=0.5):\n\n    selected = base_features\n\n    X_total = data.copy()\n    y = X_total.pop('sale_price')\n    all_features = list(X_total.columns)\n\n    base_score = 0\n\n    while True:\n\n        best_score = 0\n        best_feature = None\n\n        # Teste alle noch nicht gewählten Features\n        for col in all_features:\n            if col not in selected:\n\n                features = selected + [col]\n                X = X_total[features]\n\n                X_train, X_test, y_train, y_test = train_test_split(\n                    X, y, test_size=0.3, random_state=1\n                )\n\n                preprocessor = ColumnTransformer([\n                    ('num', StandardScaler(), Selector(dtype_include='number')),\n                    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False),\n                     Selector(dtype_include='object'))\n                ])\n\n                model = Ridge(alpha=alpha)\n                reg = TransformedTargetRegressor(\n                    regressor=model,\n                    func=np.log1p,\n                    inverse_func=np.expm1\n                )\n\n                pipe = make_pipeline(preprocessor, reg)\n                pipe.fit(X_train, y_train)\n                preds = pipe.predict(X_test)\n                score = r2_score(y_test, preds)\n\n                if score > best_score:\n                    best_score = score\n                    best_feature = col\n\n        if best_score > base_score:\n            selected.append(best_feature)\n            base_score = best_score\n        else:\n            break\n\n    return (selected + ['sale_price'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:51:50.742163Z","iopub.execute_input":"2025-12-10T10:51:50.742469Z","iopub.status.idle":"2025-12-10T10:51:50.753467Z","shell.execute_reply.started":"2025-12-10T10:51:50.742446Z","shell.execute_reply":"2025-12-10T10:51:50.751673Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Als nächstes wenden wir die Funktionen an, um die optimalen Features zu bestimmen, das beste alpha für die ermittelten Features, sowie die Modellgüte.  \nErgebnis wird ein lokales optimum sein, also nicht unbedingt die beste Kombination aus Hyperparameter und Features.","metadata":{}},{"cell_type":"code","source":"base_features = [b for b in base if b not in ['sale_price']]\nfeatures = get_features(df_fe, base_features = base_features)\nfeatures","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:51:56.253452Z","iopub.execute_input":"2025-12-10T10:51:56.253777Z","iopub.status.idle":"2025-12-10T10:52:48.222360Z","shell.execute_reply.started":"2025-12-10T10:51:56.253756Z","shell.execute_reply":"2025-12-10T10:52:48.217253Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['kms_run',\n 'transmission',\n 'variant',\n 'registered_state',\n 'make',\n 'model',\n 'total_owners',\n 'car_rating',\n 'age',\n 'body_type',\n 'avg_rating_by_model',\n 'rto_state',\n 'fuel_type',\n 'age_vs_model_avg',\n 'kms_per_age',\n 'avg_age_by_model',\n 'kms_per_owner',\n 'sale_price']"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"get_alpha_ridge(df_fe[features], low = -1, high = 1, n = 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:53:06.615539Z","iopub.execute_input":"2025-12-10T10:53:06.615854Z","iopub.status.idle":"2025-12-10T10:54:56.753087Z","shell.execute_reply.started":"2025-12-10T10:53:06.615831Z","shell.execute_reply":"2025-12-10T10:54:56.752260Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'transformedtargetregressor__regressor__alpha': 0.7196856730011519}"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"result_5 = get_ridge(df_fe[features], alpha =0.72, return_mae = True)\nresult_5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T10:55:02.670339Z","iopub.execute_input":"2025-12-10T10:55:02.671621Z","iopub.status.idle":"2025-12-10T10:55:07.435351Z","shell.execute_reply.started":"2025-12-10T10:55:02.671572Z","shell.execute_reply":"2025-12-10T10:55:07.434539Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(0.9043663582952732, 45783.12850969297)"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"Regularisierung führt in diesem Fall zu einer deutlich besseren Modellperformance.\nNeben Ridge (L2-Regularisierung) könnte auch eine Lasso-Regression (L1-Regularisierung) eingesetzt werden, die Koefﬁzienten mit sehr geringem Einfluss vollständig auf 0 setzen kann und so eine automatische Feature-Selektion ermöglicht.\n\nFür dieses Projekt habe ich mich jedoch bewusst für Ridge entschieden, da es bei stark korrelierten Features stabilere Ergebnisse liefert und in der Praxis häufig die bessere Performance sowie kürzere Berechnungszeiten bietet.","metadata":{}},{"cell_type":"markdown","source":"## Übersicht & Interpretation\n#### Übersicht Modellperformance\nHier trage ich die Zwischenergebnisse in eine Tabelle ein, um zu vergleichen, welche Performance die einzelnen Schritte gebracht haben.","metadata":{}},{"cell_type":"code","source":"steps = [\n    ('Regression ohne FE', result_1),\n    ('Zusätzliche Features', result_2),\n    ('Binning', result_3),\n    ('Log-Transformation des Targets', result_4),\n    ('Ridge Regression', result_5)\n]\n\nrows = []\nfor name, (r2, mae) in steps:\n    rows.append((name, round(r2, 4), round(mae, 0)))\n\ndf_results = pd.DataFrame(rows, columns=['Modellschritt', 'R² (CV)', 'MAE (CV)'])\ndf_results.index = df_results.index + 1\ndf_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:00:44.502591Z","iopub.execute_input":"2025-12-10T11:00:44.502912Z","iopub.status.idle":"2025-12-10T11:00:44.521529Z","shell.execute_reply.started":"2025-12-10T11:00:44.502891Z","shell.execute_reply":"2025-12-10T11:00:44.519351Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                    Modellschritt  R² (CV)  MAE (CV)\n1              Regression ohne FE   0.8775   49340.0\n2            Zusätzliche Features   0.8788   49052.0\n3                         Binning   0.8818   48853.0\n4  Log-Transformation des Targets   0.8952   47025.0\n5                Ridge Regression   0.9044   45783.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Modellschritt</th>\n      <th>R² (CV)</th>\n      <th>MAE (CV)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Regression ohne FE</td>\n      <td>0.8775</td>\n      <td>49340.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Zusätzliche Features</td>\n      <td>0.8788</td>\n      <td>49052.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Binning</td>\n      <td>0.8818</td>\n      <td>48853.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Log-Transformation des Targets</td>\n      <td>0.8952</td>\n      <td>47025.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Ridge Regression</td>\n      <td>0.9044</td>\n      <td>45783.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"Das bedeutet, dass unser Ridge Modell 90,44% der Varianz des Verkauspreises erklärt. Im Schnitt weichen die Schätzungen um 45783 Einheiten (vermutlich indische Rupien) vom tatsächlichen Preis des Autos ab.","metadata":{}},{"cell_type":"markdown","source":"#### Interpretation kategorischer Koeffizienten\nZuerst extrahiere ich die Koeffizienten der Ridge-Regression. Dafür nutze ich die zuvor definierte Funktion **get_ridge**. Anschließend sortiere ich die erzeuge Series in kleinere Series, je nach Feature, dem sie zugehörig sind.","metadata":{}},{"cell_type":"code","source":"important_features = get_ridge(df_fe[features], alpha = 0.72, return_coef = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:00:46.597263Z","iopub.execute_input":"2025-12-10T11:00:46.597635Z","iopub.status.idle":"2025-12-10T11:00:49.181288Z","shell.execute_reply.started":"2025-12-10T11:00:46.597611Z","shell.execute_reply":"2025-12-10T11:00:49.180517Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"#Aufteilen der Koeffizienten je nach Feature\nVariant = important_features[important_features.index.str.startswith('cat__variant')]\nModel = important_features[important_features.index.str.startswith('cat__model')]\nMake = important_features[important_features.index.str.startswith('cat__make')]\nTransmission = important_features[important_features.index.str.startswith('cat__transmission')]\nRegistered_state = important_features[important_features.index.str.startswith('cat__registered_state')]\nCar_rating = important_features[important_features.index.str.startswith('cat__car_rating')]\nBody_type = important_features[important_features.index.str.startswith('cat__body_type')]\nFuel_type = important_features[important_features.index.str.startswith('cat__fuel_type')]\nRto_state = important_features[important_features.index.str.startswith('cat__rto_state')]\nNums = important_features[important_features.index.str.startswith('num__')]\n\nbetas_cat = [\n    (\"Variant\", Variant),\n    (\"Model\", Model),\n    (\"Make\", Make),\n    (\"Transmission\", Transmission),\n    (\"Registered_state\", Registered_state),\n    (\"Car_rating\", Car_rating),\n    (\"Body_type\", Body_type),\n    (\"Fuel_type\", Fuel_type)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:00:56.336181Z","iopub.execute_input":"2025-12-10T11:00:56.336544Z","iopub.status.idle":"2025-12-10T11:00:56.354579Z","shell.execute_reply.started":"2025-12-10T11:00:56.336517Z","shell.execute_reply":"2025-12-10T11:00:56.353268Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"Die Koeffizienten, mit denen die Kategorien in unser Modell einfließen, sind jeweils als Faktor zu interpretieren, der auf die Referenzkategorie des selben Features angewendet wird. Diese Referenzkategorie taucht nicht unter den Koeffizienten auf, da sie durch drop = 'frist' beim One-Hot-Encoding entfernt wurden. Verwenden wir also erneut **get_ridge** um diese Referenzkategorien anzuzeigen.","metadata":{}},{"cell_type":"code","source":"get_ridge(df_fe[features], alpha = 0.72, return_reference = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:01:00.526242Z","iopub.execute_input":"2025-12-10T11:01:00.526595Z","iopub.status.idle":"2025-12-10T11:01:03.333961Z","shell.execute_reply.started":"2025-12-10T11:01:00.526571Z","shell.execute_reply":"2025-12-10T11:01:03.332039Z"}},"outputs":[{"name":"stdout","text":"Referenzkategorien:\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"            Feature    Referenzkategorie\n1      transmission            automatic\n2           variant  1.0 climber opt amt\n3  registered_state                Other\n4              make                Other\n5             model             3 series\n6        car_rating                 fair\n7         body_type            hatchback\n8         rto_state                   ap\n9         fuel_type               diesel","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Referenzkategorie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>transmission</td>\n      <td>automatic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>variant</td>\n      <td>1.0 climber opt amt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>registered_state</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>make</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>model</td>\n      <td>3 series</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>car_rating</td>\n      <td>fair</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>body_type</td>\n      <td>hatchback</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rto_state</td>\n      <td>ap</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fuel_type</td>\n      <td>diesel</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"Als Nächstes stelle ich pro Feature jeweils den höchsten und niedrigsten Koeffizienten, mit denen Kategorien ins Modell einfließen kompakt in einem Dataframe dar. Da das Target logarithmiert wurde, wenden wir exp()-1 an und multiplizieren das mit 100 um einen prozentualen Aufschlag auf die Referenzkategorie als interpretierbares Ergebnis zu bekommen.","metadata":{}},{"cell_type":"code","source":"def clean(cat_name):\n    return cat_name.split('__', 1)[1].split('_', 1)[1].replace('_', ' ')\n\ntable = PrettyTable()\ntable.field_names = [\n    'Feature',\n    'Highest_Category', 'Coef_Highest',\n    'Lowest_Category', 'Coef_Lowest'\n]\n\nfor name, series in betas_cat:\n\n    highest_cat_raw = series.idxmax()\n    lowest_cat_raw = series.idxmin()\n\n    table.add_row([\n        name,\n        clean(highest_cat_raw), f'{round(np.expm1(series.max())*100, 2)} %',\n        clean(lowest_cat_raw),  f'{round(np.expm1(series.min())*100, 2)} %'\n    ])\n\ntable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:01:19.898773Z","iopub.execute_input":"2025-12-10T11:01:19.899129Z","iopub.status.idle":"2025-12-10T11:01:19.912348Z","shell.execute_reply.started":"2025-12-10T11:01:19.899096Z","shell.execute_reply":"2025-12-10T11:01:19.911349Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"+------------------+------------------------+--------------+-----------------------+-------------+\n|     Feature      |    Highest_Category    | Coef_Highest |    Lowest_Category    | Coef_Lowest |\n+------------------+------------------------+--------------+-----------------------+-------------+\n|     Variant      | 3.0 v 6 premium luxury |   125.83 %   | c 220 cdi elegance mt |   -48.13 %  |\n|      Model       |          zen           |   82.48 %    |          nano         |   -50.4 %   |\n|       Make       |     mercedes benz      |   71.21 %    |         datsun        |   -33.32 %  |\n|   Transmission   |        missing         |   -8.73 %    |         manual        |   -16.08 %  |\n| Registered_state |    state telangana     |   31.57 %    |   state maharashtra   |   -7.36 %   |\n|    Car_rating    |      rating good       |    7.9 %     |      rating great     |    1.34 %   |\n|    Body_type     |   type luxury sedan    |   102.82 %   |       type sedan      |    8.05 %   |\n|    Fuel_type     |     type electric      |   -8.39 %    |   type petrol & cng   |   -14.09 %  |\n+------------------+------------------------+--------------+-----------------------+-------------+","text/html":"<table>\n    <thead>\n        <tr>\n            <th>Feature</th>\n            <th>Highest_Category</th>\n            <th>Coef_Highest</th>\n            <th>Lowest_Category</th>\n            <th>Coef_Lowest</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Variant</td>\n            <td>3.0 v 6 premium luxury</td>\n            <td>125.83 %</td>\n            <td>c 220 cdi elegance mt</td>\n            <td>-48.13 %</td>\n        </tr>\n        <tr>\n            <td>Model</td>\n            <td>zen</td>\n            <td>82.48 %</td>\n            <td>nano</td>\n            <td>-50.4 %</td>\n        </tr>\n        <tr>\n            <td>Make</td>\n            <td>mercedes benz</td>\n            <td>71.21 %</td>\n            <td>datsun</td>\n            <td>-33.32 %</td>\n        </tr>\n        <tr>\n            <td>Transmission</td>\n            <td>missing</td>\n            <td>-8.73 %</td>\n            <td>manual</td>\n            <td>-16.08 %</td>\n        </tr>\n        <tr>\n            <td>Registered_state</td>\n            <td>state telangana</td>\n            <td>31.57 %</td>\n            <td>state maharashtra</td>\n            <td>-7.36 %</td>\n        </tr>\n        <tr>\n            <td>Car_rating</td>\n            <td>rating good</td>\n            <td>7.9 %</td>\n            <td>rating great</td>\n            <td>1.34 %</td>\n        </tr>\n        <tr>\n            <td>Body_type</td>\n            <td>type luxury sedan</td>\n            <td>102.82 %</td>\n            <td>type sedan</td>\n            <td>8.05 %</td>\n        </tr>\n        <tr>\n            <td>Fuel_type</td>\n            <td>type electric</td>\n            <td>-8.39 %</td>\n            <td>type petrol &amp; cng</td>\n            <td>-14.09 %</td>\n        </tr>\n    </tbody>\n</table>"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"- Im Vergleich zur Referenzvariante **1.0 climber opt amt** ist **3.0 v 6 premium luxury** im Schnitt um 126% teurer\n- **c 220 cdi elegance mt** hingegen ist um 48% billiger\n- verglichen mit dem Modell **3 series** ist **zen um 82% teurer während **nano** um 50% günstiger ist\n- **mercedes benz** ist die teuerste Marke und erwirkt einen Aufschlag von 72% auf die Referenz\n- Schaltfahrzeuge **manual** sind um 16% billiger als die Referenz **automatic**\n- Überraschenderweiße sind elektrische Fahrzeuge **electric** um 8% billiger als Dieselfahrzeuge, dies liegt wohl an der geringen Datenmenge für electric","metadata":{}},{"cell_type":"markdown","source":"#### Interpretation numerischer Koeffizienten\nWerfen wir nun einen Blick auf unsere numerischen Features. Zusätzlich zu exp()-1 muss hier beachtet werden, dass die Werte vor dem training unseres Modells standardisiert wurden. Die Koeffizienten sind also als Änderung pro Standardabweichung zu interpretieren. Druch Anwendung von expm1() und Teilen durch die Standardabweichung erhalten wir eine prozentuale Änderung pro erhöhung einer Einhait eines gegebenen numerischen Features.","metadata":{}},{"cell_type":"code","source":"df_nums = (\n    Nums.rename_axis('feature')\n        .reset_index()\n)\n\ndf_nums['feature'] = df_nums['feature'].str.replace(r'^num__', '', regex = True)\ndf_nums.columns = ['feature', 'coef']\ndf_nums.index = df_nums.index + 1\ndf_nums['coef'] = round(df_nums['coef'], 4)\n\nnums = df_nums['feature'].tolist()\n\n\nstd_series = df_fe[nums].std()\ndf_nums['std'] = std_series.values\n\n\npct = np.expm1(df_nums['coef'] / df_nums['std']) * 100\n\ndf_nums['perc_change_per_unit'] = pct.apply(lambda x: f\"{x:.6f} %\")\n\n\ndf_nums = df_nums.drop(columns='std')\n\ndf_nums\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T11:01:39.560282Z","iopub.execute_input":"2025-12-10T11:01:39.560628Z","iopub.status.idle":"2025-12-10T11:01:39.580547Z","shell.execute_reply.started":"2025-12-10T11:01:39.560604Z","shell.execute_reply":"2025-12-10T11:01:39.579212Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"               feature    coef perc_change_per_unit\n1                  age -0.3430         -10.542055 %\n2     age_vs_model_avg  0.0564          20.924984 %\n3              kms_run -0.0372          -0.000085 %\n4  avg_rating_by_model  0.0334          14.659222 %\n5         total_owners -0.0321          -5.395883 %\n6     avg_age_by_model -0.0099          -0.424856 %\n7        kms_per_owner -0.0086          -0.000022 %\n8          kms_per_age  0.0074           0.000113 %","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>coef</th>\n      <th>perc_change_per_unit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>age</td>\n      <td>-0.3430</td>\n      <td>-10.542055 %</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>age_vs_model_avg</td>\n      <td>0.0564</td>\n      <td>20.924984 %</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>kms_run</td>\n      <td>-0.0372</td>\n      <td>-0.000085 %</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>avg_rating_by_model</td>\n      <td>0.0334</td>\n      <td>14.659222 %</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>total_owners</td>\n      <td>-0.0321</td>\n      <td>-5.395883 %</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>avg_age_by_model</td>\n      <td>-0.0099</td>\n      <td>-0.424856 %</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>kms_per_owner</td>\n      <td>-0.0086</td>\n      <td>-0.000022 %</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>kms_per_age</td>\n      <td>0.0074</td>\n      <td>0.000113 %</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"- pro zusätzlichem Jahr sinkt also der Preis um etwa 11%\n- 1000 zusätzliche gefahrene Kilometer verringern den Preis um -0.085%\n- Jeder weitere Besitzer verringert den Preis um etwa 5.4%","metadata":{}},{"cell_type":"markdown","source":"#### Fazit\n\nIn diesem Notebook habe ich ein lineares Basismodell entwickelt und schrittweise durch sinnvolle Feature-Engineering-Methoden und Regularisierung verbessert. Die log-transformierte Ridge-Regression erzielte dabei die stabilsten und genauesten Ergebnisse und dient als interpretierbares Referenzmodell für die weitere Modellierung.\n\nEs wurde aber deutlich, dass das lineare Modell bestimmte nichtlineare Muster nur eingeschränkt erfassen kann und nur bedingt für hochkardinale Daten geeignet sind.\nIm nächsten Notebook werde ich daher Gradient-Boosting-Modelle einsetzen, um komplexere Interaktionen und Nichtlinearitäten abzubilden und die Vorhersagegenauigkeit weiter zu steigern.","metadata":{}}]}